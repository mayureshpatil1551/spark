ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. what is diff between worker node and container?
-->
Worker Node
 â”œâ”€â”€ Container 1 â†’ Executor 1
 â”œâ”€â”€ Container 2 â†’ Executor 2
 â””â”€â”€ Container 3 â†’ Executor 3
*Spark executor runs inside a container which is allocated on a worker node.
* A worker node is a physical or virtual machine in the cluster, while a container is a Isolated virtual runtime env It comes with some CPU & memory allocation.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is py4j connection. 
-->
Python Process (PySpark)
        |
        |  Py4J (Socket / Gateway)
        |
JVM Process (Spark Driver / Executor)

* One Py4J gateway per driver
* Executors also use Py4J when Python UDFs are involved.
* Py4J is a communication gateway that allows PySparkâ€™s Python process to interact with Sparkâ€™s JVM-based execution engine by sending commands and receiving results over a socket connection.

** The python code is design to start java mai () method internally. So, my pyspark appln start JVM appln. 
After that python wapper will call java wrapper usingPy4J connection.
Py4J is allow pyspark appln to to call Java appln & that's how pyspark work. It will start JVM appln and call spark API's in the JVM.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. How we identify, are we using python worker in spark or not?
-->
Spark uses a Python worker whenever Python code such as RDD operations, Python UDFs, or Pandas UDFs need to be executed on executors; 
this can be identified through code usage, Spark UI executor details, or executor logs showing Python Runner activity.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. How we can calculate no of drivers and executor for our data for spark submit?
-->
There is always exactly ONE driver per Spark application
So the real question is:
Q. How do we calculate number of executors, cores, and memory?
We donâ€™t calculate drivers since Spark has one driver per application. Executors are calculated based on available cluster resources by reserving memory and cores for the OS, choosing optimal cores per executor (usually 3â€“5), and dividing remaining resources to determine executors per node and total executors.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Spark driver Vs spark executor?
-->
What does Spark Driver do?
The Driver runs on the client or cluster and is responsible for:
* Creates Spark Context / Spark Session
	- Spark Context: Low-level entry point that manages cluster resources and executes RDD-based operations.
	- Spark Session: Unified, high-level entry point (Spark 2+) that wraps SparkContext and provides APIs for SQL, DataFrames, and datasets.
* Converts code into DAG
* Breaks DAG into stages
* Schedules tasks to executors
* Tracks execution & failures
* Collects results (if needed)
- Driver never processes data directly (except very small actions like collect())

What does Spark Executor do?
Executors are JVM processes launched on worker nodes that:
* Run tasks
* Process data partitions
* Perform transformations
* Cache data (if persisted)
* Send results & metrics back to driver
- Executors live for the entire application (unless dynamic allocation is used)

** The Spark driver is the central coordinator that builds the execution plan, schedules tasks, and monitors execution, 
while Spark executors are distributed JVM processes on worker nodes that execute tasks on data partitions and return results to the driver.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰
Q. Explain Spark execution plan?
--> When u pass analyses phase means u hv valid code.
then they apply optimization logical plan to our code & create optimized logical plan.
That plan taken by spark engine, it will create one or more physical plan and apply cost base optimization 
and select plan with at least cost.
they use different algorithm to create more than one physical plan like broadcast join, shuffler hash join.
then they apply cost to each plan and chose best one. then plan will go to code generation.
The engine will generate java byte code for RDDs.
that all about spark SQL engine. 

*-->
- A Spark Execution Plan is the step-by-step plan Spark creates to convert your code into distributed execution on executors.

User Code
   |
   â†“ 
Logical Plan [Spark check syntax of your Data Frame / SQL code]
   |
   | (Catalyst Optimizer) 
   â†“ 
Optimized Logical Plan [Spark applies rule-based optimizations. Predicate pushdown, Reordering filters]
   |
   â†“ 
Physical Plan
   |            [
   |            Now Spark decides HOW to execute:
   |            * Which join strategy?
   |                    -Broadcast
   |                    -Sort-merge
   |            * Where to shuffle
   |            * How many tasks ]
   |
   â†“ 
DAG â†’ Stages â†’ Tasks 
[DAG - Spark converts the physical plan into a DAG of transformations.]
[stage - Spark splits DAG into stages based on shuffle boundaries.]
[Tasks -
        * Each stage is divided into tasks
        * One task per partition
        * Tasks are sent to executors     ]
   |
   â†“ 
Executors


** Spark execution plan is the multi-step process where Spark converts user code into logical and physical plans, 
optimizes them using the Catalyst optimizer, builds a DAG, splits it into stages and tasks, 
and finally executes them in parallel on executors.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. How the data moving from one stage to another stage?
-->
short [Data moves between stages through a SHUFFLE. No shuffle = no new stage.]
*Thatâ€™s why shuffle is expensive:
-  Shuffle is costly due to disk spills, cross-node network movement, and CPU overhead from serialization.
-  Disk I/O
-  Network transfer
-  Serialization / deserialization

Q.How to IDENTIFY stage-to-stage movement?
-->
Spark UI
-- Look for Shuffle Read / Shuffle Write
-- New stage = shuffle boundary
Execution plan
-  Look for Exchange operator

** Data moves from one stage to another in Spark through a shuffle operation, 
where intermediate data is serialized and written to disk by tasks in the previous stage and 
then fetched over the network by tasks in the next stage.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Difference between stage and task?
-->
* A stage is a logical group of task, a task is the actual unit of execution.
* Stages are created by the Driver.
* 100 partitions â†’ 100 tasks

Job
 â””â”€â”€ Stage
      â””â”€â”€ Tasks (one per partition)

* stage is create o driver.
* task Runs on Executors

** A stage is a logical group of transformations separated by shuffle boundaries, 
while a task is the smallest unit of execution that processes a single data partition 
and runs in parallel on executors.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Task vs thread vs core
-->
* Task = work
* Thread = how work runs
* Core = how much work can run in parallel
   executor-cores = 4
   â†’ Executor can run 4 tasks simultaneously

* Partitions â†’ Tasks â†’ Threads â†’ Cores
* Executor (4 cores)
   â”œâ”€â”€ Thread 1 â†’ Task 1
   â”œâ”€â”€ Thread 2 â†’ Task 2
   â”œâ”€â”€ Thread 3 â†’ Task 3
   â””â”€â”€ Thread 4 â†’ Task 4

Q. Why this matters in tuning (real-world)
* Too few tasks â†’ cores idle
* Too many tasks â†’ scheduling overhead
* Too many cores per executor â†’ GC[garbage collection] pressure
* That's why:
- 3-5 cores per executor is recommended

* Executor runs on a Worker Node.

** In Spark, a task is the smallest unit of work that processes one partition, 
a thread is the JVM execution unit that runs the task, 
and a core is a CPU resource that determines how many tasks can run in parallel on an executor.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Difference between cluster manager and spark driver?
-->
* Cluster Manager gives resources. --> YARN, Spark standalone, Mesos, Kubernetes
* Spark Driver uses those resources to run the job. --> Spark application

spark-submit
   â†“
Spark Driver starts
   â†“
Driver asks Cluster Manager for resources
   â†“
Cluster Manager allocates containers
   â†“
Executors start on worker nodes
   â†“
Driver schedules tasks on executors

** The cluster manager is responsible for allocating and managing cluster resources, 
while the Spark driver is responsible for building the execution plan, scheduling tasks, 
and coordinating the execution of a Spark application using those resources.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is meager difference between spark and Hadoop MapReduce?
-->
* MapReduce is disk-based batch processing.
* Spark is in-memory, DAG-based distributed processing.
* Spark can run on HDFS
* Spark often replaces MapReduce, not Hadoop ecosystem

** The major difference is that Hadoop MapReduce is a disk-based batch processing framework with a fixed Map and Reduce model, 
while Spark uses an in-memory, DAG-based execution engine that supports faster, iterative(repeated task), and interactive data processing.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain DAG (Directed Acyclic Graph) ?
-->
* In Spark, a DAG is a logical execution plan that represents all transformations and actions in your job and their dependencies.

* Spark uses DAG to:
- Combine multiple operations into one job
- Avoid unnecessary disk I/O
- Optimize execution
- This is why Spark is faster.

Read â†’ Map â†’ Filter â†’ Shuffle â†’ Reduce
DAG
 â””â”€â”€ Stage
      â””â”€â”€ Tasks

Common interview mistakes âŒ
- DAG executes tasks.
- DAG is created per stage.
- DAG is physical execution.

** In Spark, a DAG is a directed acyclic graph that represents the logical execution plan of transformations and actions, 
capturing dependencies between operations and enabling Spark to optimize, schedule stages, and execute tasks efficiently.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰


Q. Explain Spark executor memory allocation and use of each parameter in short?
-->
spark.memory.offHeap.enabled  --> false (not enabled by default) 
spark.memory.offHeap.size     --> 0 (no default value; irrelevant unless enabled) 
spark.executor.pyspark.memory --> Not defined (no default in docs)
spark.executor.memory         --> 1g 
spark.executor.memoryOverhead --> max(384 MB, 10% of executor.memory)

-->
* Total executor memory = JVM heap + overhead

* --executor-memory 8G # Main JVM heap memory for executor [data processing,cache/persist, shiffle, UDF execution]
* spark.executor.memoryOverhead # Off-heap + non-JVM memory
* spark.memory.fraction [How much want memory want to run DF operations
   - Fraction of heap for execution + storage
   - Default: 0.6
* spark.memory.storageFraction
   - Storage share inside memory fraction
   - Default: 0.5

* Memory layout -
  Executor JVM Heap
   â”œâ”€â”€ Execution + Storage (60%)
   â”‚     â”œâ”€â”€ Storage (30%)    # Joins,Aggregations,Shuffles
   â”‚     â””â”€â”€ Execution (30%)  # Cached / persisted data
   â””â”€â”€ User / Reserved Memory (40%)



** Spark executor memory consists of JVM heap defined by spark.executor.memory for execution and storage, 
plus off-heap memory defined by spark.executor.memoryOverhead, with memory fractions controlling 
how heap is shared between execution and cached data.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰


Q. What will happen if we give more than 5 executor.core?
-->
* Spark will still run, but performance usually degrades.
* Each task runs in one thread
* If it crashes â†’ many tasks fail at once

* More GC â†’ Less task execution â†’ Slower job
* GC - Garbage Collection is a JVM mechanism that reclaims unused memory objects, 
      and in Spark excessive GC overhead occurs 
      when executors spend more time cleaning memory than executing tasks, 
      it will couse performance degradation.

** Using more than 5 executor cores increases JVM garbage collection overhead, 
thread contention, and memory pressure, which usually degrades Spark performance; 
hence, 3-5 cores per executor is considered optimal.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is Adaptive Query Execution(AQE) in short?
-->
It does -
* Changes join strategy on the fly (e.g., Sort-Merge â†’ Broadcast)
* Handles data skew by splitting skewed partitions
* Coalesces shuffle partitions to reduce small tasks

** AQE dynamically adjusts Sparkâ€™s physical execution plan at runtime based on real data statistics to improve performance.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰
Q. Explain Dynamically coalescing shuffle partitions.
--> combine andn coalesce the small partitions.
we need to give,
Enable AQE
max num of shuffle partitions,
min num of shuffle partitions
each partition size (64 MB default)
Enable cealsce partition (for to make combine/small partitions)

-->
* Spark automatically reduces the number of shuffle partitions at runtime when it detects that shuffle data is smaller than expected.
* default on in spark 3+
  >> spark.sql.adaptive.enabled = true
  >> spark.sql.adaptive.coalescePartitions.enabled = true
  >> spark.sql.adaptive.advisoryPartitionSizeInBytes = 64MB

** Dynamically coalescing shuffle partitions is an AQE optimization where Spark reduces the number of shuffle partitions at runtime by merging small partitions 
based on actual shuffle data size, thereby minimizing task overhead and 
improving performance.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Dynamically switching join strategies.
-->
* Spark changes the join algorithm at runtime based on the actual size of the join inputs, instead of sticking to the join chosen during query planning.

  >> spark.sql.adaptive.enabled = true
  >> spark.sql.adaptive.localShuffleReader.enabled = true
  >> spark.sql.autoBroadcastJoinThreshold = 10MB

** Dynamically switching join strategies is an AQE feature where Spark changes 
the join algorithm at runtimeâ€”such as switching from a sort-merge join to 
a broadcast joinâ€”based on actual data sizes to reduce shuffle and improve performance.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Dynamically Optimizing skew joins
-->
Spark detects skewed join partitions at runtime and splits them into smaller sub-partitions 
so, work is balanced across executors.

  >> spark.sql.adaptive.enabled = true
  >> spark.sql.adaptive.skewJoin.enabled = true
  >> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes
  >> spark.sql.adaptive.skewJoin.skewedPartitionFactor

** Dynamically optimizing skew joins is an AQE feature where Spark detects skewed join partitions at runtime and 
splits them into smaller partitions so that join work is evenly distributed across executors, reducing straggler tasks.

** When AQE optimizes skew joins, Spark splits the skewed partition into multiple smaller tasks, 
which can be scheduled on different executors to balance the workload and avoid stragglers.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

video done
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is Dynamic Partition Pruning?
Q. Why do we need Dynamic Partition Pruning?
-->
spark.sql.optimizer.dynamicPartitionPruning.enable = true # [Enabled by default in Spark 3+ , Works best with AQE]


pruning - removing unnecessary part ( cut karane)
** Pruning means avoiding unnecessary data reads by skipping files, partitions, or rows that are not needed for a query.
** Dynamic Partition Pruning is a runtime optimization where Spark prunes partitions dynamically based on values produced during query execution.
** Dynamic Partition Pruning is needed when partition filter values are not known at query planning time, so Spark can avoid scanning unnecessary partitions.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain caching in Apache Spark?
-->
Caching stores intermediate data (RDD/DataFrame) in memory so Spark can reuse it across multiple actions without recomputing from the source.
Why caching is needed -
	- Spark recomputes lineage by default
	- Repeated actions = repeated computation
	- Caching avoids recomputation â†’ faster jobs
How it works - 
	- Call cache() or persist()
	- Data is cached after the first action
	- Next actions reuse cached data

Storage levels (quick)
	- MEMORY_ONLY (default)
	- MEMORY_AND_DISK
	- DISK_ONLY
	- Serialized versions (_SER)

When to use cache - 
	- Reused DataFrames
	- Iterative algorithms
	- Multiple actions on same data
	- Expensive transformations

When NOT to use -
	- One-time use data
	- Very large datasets that donâ€™t fit memory

** Caching in Spark stores reusable datasets in memory to avoid re-computation and improve performance across multiple actions.


Q How do you cache DF in executor stg pool?
-->
By calling cache() or persist(), Spark stores the DataFrame in the executorâ€™s storage memory pool (not on the driver).

What is the executor storage pool?
	Each executor JVM has memory split into:
	Execution memory â†’ shuffles, joins, aggregations
	Storage memory â†’ cached / persisted data
	(Managed by Unified Memory Manager)
Cached DataFrames live in the storage region of executors.


How caching actually works (step-by-step)

>> df.cache()
>> # or
>> df.persist(StorageLevel.MEMORY_ONLY)


cache() marks the DF as cacheable (lazy)
2. First action triggers computation
3. Each executor:
	- Computes its partition
	- Stores that partition in its local storage pool
4. Metadata about cached blocks is tracked by the driver
5. Future actions:
	- Executors read from local cache
	- No re-computation


Important points (interview gold)
Cached data is:
	Distributed across executors
	Stored partition-wise
Driver never stores cached data
If storage memory is full:
	Blocks may be evicted (LRU)
	Or spilled to disk (based on storage level)

Example with storage level
>> from pyspark import StorageLevel
>> df.persist(StorageLevel.MEMORY_AND_DISK)

Memory full â†’ spill to disk
Still stored at executor level


How to verify in Spark UI :-
Spark UI â†’ Storage tab
Shows:
	Cached RDD/DF
	Memory used per executor
	Storage level

** A Data Frame is cached in Spark by calling cache() or persist(), which stores its partitions in the executorâ€™s 
storage memory pool after the first action, 
allowing reuse without re-computation.

When you want to access large DF multiple times across spark actions, you should consider caching or database.
But make sure, you know your memory requirements & configure your memory accordingly.

If you are not reusing your DF frequently or too small, do not cache them.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Repartition and Coalesce?
-->
Default Repartition size is 200.

Repartition :-
	- Changes the number of partitions using a full shuffle
	- Can increase or decrease partitions
	- Redistributes data evenly
 	- Expensive due to shuffle
>> df.repartition(200)
>> df.repartition(200, "key")

Coalesce :-
	- Reduces partitions by merging existing partitions
	- Avoids full shuffle
	- Faster but may create uneven partitions
	- Can only decrease partitions
>> df.coalesce(50)

partitionBy :-
Â 	- Used only while writing data
Â 	- A write option, not a transformation
Â 	- Physically partitions data on disk
Â 	- Creates directory structure
Â 	- Causes shuffle during write
>> df.write.partitionBy("country").parquet(path)


**
Repartition - reshuffles data to change partitions evenly. In-memory redistribution
Coalesce - reduces partitions by merging them without a full shuffle.
partitionBy - physically organizes data into directory partitions on disk during write for faster reads. On-disk partitioning

>> df.repartition(200).write.partitionBy("dt").parquet(path)

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. When should you use repartition?

repartition -
	Use when Need better parallelism
	Before joins or aggregations
	Fixing data skew

coalesce-
	Use when After filtering data
	Use when Before writing output
	Use when Reducing small files



ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰


Q. Give me real project example for repartition and coalesce?
-->
In real projects, repartition is used before joins or aggregations to improve parallelism and balance data,
while coalesce is used before writing data to reduce the number of output files and avoid small file problems.

âš ï¸ Common Production Mistakes âŒ
Using repartition before write (slow)
Using coalesce before join (skew risk)
Hardcoding partition numbers without cluster awareness

ğŸ§  Pro Tip (Senior-level)
Let AQE handle repartition when possible
Use manual repartition only when:
	Skew exists
	Parallelism is clearly insufficient

-- 20 GB data
sales_df = spark.read.parquet("/raw/sales")
cust_df  = spark.read.parquet("/dim/customer")
sales_df = sales_df.repartition(400, "customer_id")
result = (
    sales_df
    .join(cust_df, "customer_id")
    .groupBy("region")
    .sum("amount")
)

-- 2/3 GB data
final_df = transformed_df.coalesce(20)
final_df.write.mode("overwrite").parquet("/curated/sales_summary")


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰
Q. What are DataFrame hints in Spark?
-->
	Partition Hints
	Join Hints

Q. What is a broadcast hint and when do you use it?
-->
A broadcast hint forces Spark to broadcast the smaller table to all executors to avoid shuffle, 
and is used when the dimension table is small and reused in joins.

Q. Does Spark always follow DataFrame hints?
-->
No. Spark may ignore or override hints if they cause memory issues or if AQE finds a more efficient plan.

Q. How do you verify whether a hint was applied?
-->
Check the Spark UI â†’ SQL tab â†’ Physical Plan and look for join types like BroadcastHashJoin.

Q. How do DataFrame hints interact with AQE?
-->
AQE can change or override hinted join strategies at runtime based on actual data statistics.


** DataFrame hints guide Sparkâ€™s optimizer but are not guarantees, and AQE may override them based on runtime metrics.

>> df.hint("broadcast")
>> df1.join(df2.hint("broadcast"), "id")

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Explain Broadcast variable
-->
broadcast variables are read-only and shared across all tasks.
data is sent once and reused by all tasks on that executor.
Cached in the executorâ€™s memory (worker node hosts the executor).
Broadcast variable is serialized and sent only when first accessed by a task.


-->
A broadcast variable is a read-only shared variable that Spark distributes once to all executors, 
so every task can access it locally without sending it repeatedly.

How it works (simple flow)
	Driver creates broadcast variable
	Spark sends it once to each executor
	Tasks read it locally (read-only)
>> bc_map = spark.sparkContext.broadcast(my_dict)

Used in -Lookup tables, Reference data, Small dimension data, Map-side joins

----------------------------------------------------
| Broadcast Variable      | Broadcast Join         |
| ----------------------- | ---------------------- |
| Manual                  | Automatic / hint-based |
| Read-only data          | Join optimization      |
| Used in transformations | Used in joins          |
----------------------------------------------------

** A broadcast variable is a read-only shared variable that Spark sends once to all executors so tasks can 
access common data locally without repeated network transfer.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Spark Accumulators 
-->
Accumulators are variables that executors can only add values to, and Spark collects the total on the driver, 
   mainly to count or track things for monitoring and debugging.
We use an accumulator to count how many records failed validation while processing data in Spark.
Example in words:
	Each executor increases the counter when it finds a bad record, and the driver gets the final count.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Speculative execution ?
-->
Speculative execution is a Spark feature that runs duplicate copies of slow tasks on other executors to avoid stragglers and reduce job runtime.
Eg.
	Task 5 running very slow âŒ
	Spark runs Task 5 again on another executor âœ…

>> spark.speculation = true

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Spark Dynamic Resource Allocation.
-->
Dynamic Resource Allocation lets Spark automatically add or remove executors at runtime based on workload, instead of using a fixed number of executors.

>> spark.dynamicAllocation.enabled = true
>> spark.dynamicAllocation.minExecutors = 2
>> spark.dynamicAllocation.maxExecutors = 50
>> spark.dynamicAllocation.initialExecutors = 5


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What are Spark schedulers?
-->
Spark schedulers decide which job, stage, and task runs first and how cluster resources are shared between jobs.

DAG Scheduler - Planner
Breaks jobs into stages based on shuffle dependencies.

Task Scheduler - Traffic controller
Assigns tasks to executors and manages execution.

FIFO vs FAIR
FIFO runs jobs sequentially, FAIR shares resources across jobs.

FIFO -
Default job schedular
first job gets highest priority
Consumes as must as needed
Next job gets leftover resumes.

FAIR Schedular -
spark.scheduler.mode = FAIR
Round robin slot allocation
   means assign shares cluster resources across multiple jobs, multiple jobs run together so no job waits too long.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰
