Q. what is diff between worker node and container?
-->
Worker Node
 â”œâ”€â”€ Container 1 â†’ Executor 1
 â”œâ”€â”€ Container 2 â†’ Executor 2
 â””â”€â”€ Container 3 â†’ Executor 3
*Spark executor runs inside a container which is allocated on a worker node.
* A worker node is a physical or virtual machine in the cluster, while a container is a logical allocation of CPU and memory on a worker node used to run Spark executors.


Q. What is py4j connection. 
-->
Python Process (PySpark)
        |
        |  Py4J (Socket / Gateway)
        |
JVM Process (Spark Driver / Executor)
* One Py4J gateway per driver
* Executors also use Py4J when Python UDFs are involved.
* Py4J is a communication gateway that allows PySparkâ€™s Python process to interact with Sparkâ€™s JVM-based execution engine by sending commands and receiving results over a socket connection.


Q. How we identify, are we using python worker in spark or not?
-->
Spark uses a Python worker whenever Python code such as RDD operations, Python UDFs, or Pandas UDFs need to be executed on executors; this can be identified through code usage, Spark UI executor details, or executor logs showing PythonRunner activity.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. How we can calculate no of drivers and executor for our data for spark submit?
-->
There is always exactly ONE driver per Spark application
So the real question is:
Q. How do we calculate number of executors, cores, and memory?
We donâ€™t calculate drivers since Spark has one driver per application. Executors are calculated based on available cluster resources by reserving memory and cores for the OS, choosing optimal cores per executor (usually 3â€“5), and dividing remaining resources to determine executors per node and total executors.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Spark driver Vs spark executor?
-->
What does Spark Driver do?
The Driver runs on the client or cluster and is responsible for:
* Creates SparkContext / SparkSession
* Converts code into DAG
* Breaks DAG into stages
* Schedules tasks to executors
* Tracks execution & failures
* Collects results (if needed)
- Driver never processes data directly (except very small actions like collect())

What does Spark Executor do?
Executors are JVM processes launched on worker nodes that:
* Run tasks
* Process data partitions
* Perform transformations
* Cache data (if persisted)
* Send results & metrics back to driver
- Executors live for the entire application (unless dynamic allocation is used)

** The Spark driver is the central coordinator that builds the execution plan, schedules tasks, and monitors execution, 
while Spark executors are distributed JVM processes on worker nodes that execute tasks on data partitions and return results to the driver.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰
Q. Explain Spark excution plan?
--> When u pass analyase s phase means u hv valid code.
then they apply optimization loagical plan to our code & create optimized logical plan.
Tht plan taken by spark engine, it will create one or more physical plan and apply cost base optimization.
spark engine will create multiple physical plan, calculate each physical plan cost. and select plan with at least cost.
they use different algorithm to create more than one pythical plan like breadcast join, shuffler hash join.
then thy apply cost to each plan adn chose best one. then plan will go to code generation.
The engine will generate java byte code for RDDs.
that all about spark sql engine.

*-->
- A Spark Execution Plan is the step-by-step plan Spark creates to convert your code into distributed execution on executors.

User Code
   |
   â†“ 
Logical Plan [Spark check syntax of your DataFrame / SQL code]
   |
   | (Catalyst Optimizer) 
   â†“ 
Optimized Logical Plan [Spark applies rule-based optimizations.Predicate pushdown,Reordering filters]
   |
   â†“ 
Physical Plan
   |            [
   |            Now Spark decides HOW to execute:
   |            * Which join strategy?
   |                    -Broadcast
   |                    -Sort-merge
   |            * Where to shuffle
   |            * How many tasks ]
   |
   â†“ 
DAG â†’ Stages â†’ Tasks 
[DAG - Spark converts the physical plan into a DAG of transformations.]
[stage - Spark splits DAG into stages based on shuffle boundaries.]
[Tasks -
        * Each stage is divided into tasks
        * One task per partition
        * Tasks are sent to executors     ]
   |
   â†“ 
Executors


** Spark execution plan is the multi-step process where Spark converts user code into logical and physical plans, 
optimizes them using the Catalyst optimizer, builds a DAG, splits it into stages and tasks, 
and finally executes them in parallel on executors.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. How the data moving from one stage to another stage?
-->
short [Data moves between stages through a SHUFFLE. No shuffle = no new stage.]
*Thatâ€™s why shuffle is expensive:
-  Disk I/O
-  Network transfer
-  Serialization / deserialization

Q.How to IDENTIFY stage-to-stage movement?
-->
Spark UI
-- Look for Shuffle Read / Shuffle Write
-- New stage = shuffle boundary
Execution plan
-  Look for Exchange operator

** Data moves from one stage to another in Spark through a shuffle operation, 
where intermediate data is serialized and written to disk by tasks in the previous stage and 
then fetched over the network by tasks in the next stage.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Difference between stage and task?
-->
* A stage is a logical group of work, a task is the actual unit of execution.
* Stages are created by the Driver.
* 100 partitions â†’ 100 tasks

Job
 â””â”€â”€ Stage
      â””â”€â”€ Tasks (one per partition)

* stage Runs on Driver (planning)
* stage Runs on Executors

** A stage is a logical group of transformations separated by shuffle boundaries, 
while a task is the smallest unit of execution that processes a single data partition 
and runs in parallel on executors.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q.Task vs thread vs core
-->
* Task = work
* Thread = how work runs
* Core = how much work can run in parallel
   executor-cores = 4
   â†’ Executor can run 4 tasks simultaneously

* Partitions â†’ Tasks â†’ Threads â†’ Cores
* Executor (4 cores)
   â”œâ”€â”€ Thread 1 â†’ Task 1
   â”œâ”€â”€ Thread 2 â†’ Task 2
   â”œâ”€â”€ Thread 3 â†’ Task 3
   â””â”€â”€ Thread 4 â†’ Task 4

Q. Why this matters in tuning (real-world)
* Too few tasks â†’ cores idle
* Too many tasks â†’ scheduling overhead
* Too many cores per executor â†’ GC pressure
* Thats why:
- 3-5 cores per executor is recommended

** In Spark, a task is the smallest unit of work that processes one partition, 
a thread is the JVM execution unit that runs the task, 
and a core is a CPU resource that determines how many tasks can run in parallel on an executor.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Difference between cluster manager and spark driver?
-->
* Cluster Manager gives resources. --> YARN, Spark standalon, Mesos, Kubernetes
* Spark Driver uses those resources to run the job. --> Spark application

spark-submit
   â†“
Spark Driver starts
   â†“
Driver asks Cluster Manager for resources
   â†“
Cluster Manager allocates containers
   â†“
Executors start on worker nodes
   â†“
Driver schedules tasks on executors

** The cluster manager is responsible for allocating and managing cluster resources, 
while the Spark driver is responsible for building the execution plan, scheduling tasks, 
and coordinating the execution of a Spark application using those resources.


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is mager difference between spark and hadoop mapreduce?
-->
* MapReduce is disk-based batch processing.
* Spark is in-memory, DAG-based distributed processing.
* Spark can run on HDFS
* Spark often replaces MapReduce, not Hadoop ecosystem

** The major difference is that Hadoop MapReduce is a disk-based batch processing framework with a fixed Map and Reduce model, 
while Spark uses an in-memory, DAG-based execution engine that supports faster, iterative(repeted task), and interactive data processing.
ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain DAG (Directed Acyclic Graph) ?
-->
* In Spark, a DAG is a logical execution plan that represents all transformations and actions in your job and their dependencies.

* Spark uses DAG to:
- Combine multiple operations into one job
- Avoid unnecessary disk I/O
- Optimize execution
- This is why Spark is faster.

Read â†’ Map â†’ Filter â†’ Shuffle â†’ Reduce
DAG
 â””â”€â”€ Stage
      â””â”€â”€ Tasks

Common interview mistakes âŒ
- DAG executes tasks.
- DAG is created per stage.
- DAG is physical execution.

** In Spark, a DAG is a directed acyclic graph that represents the logical execution plan of transformations and actions, 
capturing dependencies between operations and enabling Spark to optimize, schedule stages, and execute tasks efficiently.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰


Q. Explain Spark executor memory allocation and use of each parameter in short?
-->
spark.memory.offHeap.enabled  --> false (not enabled by default) 
spark.memory.offHeap.size     --> 0 (no default value; irrelevant unless enabled) 
spark.executor.pyspark.memory --> Not defined (no default in docs)
spark.executor.memory         --> 1g 
spark.executor.memoryOverhead --> max(384 MB, 10% of executor.memory)

-->
* Total executor memory = JVM heap + overhead

* --executor-memory 8G # Main JVM heap memory for executor [data processing,cache/persist, shiffle, UDF execution]
* spark.executor.memoryOverhead # Off-heap + non-JVM memory
* spark.memory.fraction
   - Fraction of heap for execution + storage
   - Default: 0.6
* spark.memory.storageFraction
   - Storage share inside memory fraction
   - Default: 0.5

* Memory layout -
  Executor JVM Heap
   â”œâ”€â”€ Execution + Storage (60%)
   â”‚     â”œâ”€â”€ Storage (30%)    # Joins,Aggregations,Shuffles
   â”‚     â””â”€â”€ Execution (30%)  # Cached / persisted data
   â””â”€â”€ User / Reserved Memory (40%)



** Spark executor memory consists of JVM heap defined by spark.executor.memory for execution and storage, 
plus off-heap memory defined by spark.executor.memoryOverhead, with memory fractions controlling 
how heap is shared between execution and cached data.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰


Q. What will happen if we give more than 5 executor.core?
-->
* Spark will still run, but performance usually degrades.
* Each task runs in one thread
* If it crashes â†’ many tasks fail at once

* More GC â†’ Less task execution â†’ Slower job
* GC - Garbage Collection is a JVM mechanism that reclaims unused memory objects, 
      and in Spark excessive GC overhead occurs 
      when executors spend more time cleaning memory than executing tasks, 
      it will couse performance degradation.

** Using more than 5 executor cores increases JVM garbage collection overhead, 
thread contention, and memory pressure, which usually degrades Spark performance; 
hence, 3-5 cores per executor is considered optimal.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. What is Adaptive Query Execution(AQE) in short?
-->
It does -
* Changes join strategy on the fly (e.g., Sort-Merge â†’ Broadcast)
* Handles data skew by splitting skewed partitions
* Coalesces shuffle partitions to reduce small tasks

** AQE dynamically adjusts Sparkâ€™s physical execution plan at runtime based on real data statistics to improve performance.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Dynamically coalescing shuffle partitions.
--> combine andn coalesce the small partitions.
we need to give,
Enable AQE
max num of shuffle partitions,
min num of shuffle partitions
each partition size (64 MB default)
Enable cealsce partition (for to make combine/small partitions)


ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Dynamically switching join strategies.

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

Q. Explain Dynamically Optimizing skew joins

ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰

